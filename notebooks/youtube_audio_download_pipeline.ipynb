{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from pydub import AudioSegment\n",
    "from pytube import YouTube\n",
    "from tqdm import tqdm\n",
    "from yt_dlp import YoutubeDL\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "# from googletrans import Translator\n",
    "# from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "# youtube = build(\"youtube\", \"v3\", developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Fetch and Display Video IDs from CSV\n",
    "\n",
    "This function, `fetch_and_print_video_ids_from_csv`, reads a CSV file containing YouTube video URLs, extracts the video IDs and titles, and displays them in a DataFrame. The function performs the following steps:\n",
    "\n",
    "1. **Read CSV File**: Reads the specified CSV file into a DataFrame.\n",
    "2. **Extract Video URLs**: Iterates through each cell in the DataFrame to find YouTube video URLs.\n",
    "3. **Avoid Duplicates**: Uses a set to keep track of seen URLs to avoid duplicates.\n",
    "4. **Fetch Video Details**: For each unique video URL, extracts the video ID and title using `yt-dlp`.\n",
    "5. **Create DataFrame**: Compiles the video IDs, URLs, and titles into a new DataFrame and displays it.\n",
    "6. **Return DataFrame**: Returns the DataFrame containing video details for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] qtRkHCFW434: nsig extraction failed: Some formats may be missing\n",
      "         n = h8nk7FdZbmJEwMu-5l3k ; player = https://www.youtube.com/s/player/f8071a08/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] qtRkHCFW434: nsig extraction failed: Some formats may be missing\n",
      "         n = ERzGwEbeMQPTROHkEtUU ; player = https://www.youtube.com/s/player/f8071a08/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] ceQCPcHrwy4: nsig extraction failed: Some formats may be missing\n",
      "         n = cUEkKltHNEg6oaZVr8WA ; player = https://www.youtube.com/s/player/f8071a08/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] ceQCPcHrwy4: nsig extraction failed: Some formats may be missing\n",
      "         n = WGNIvIuKddKQZdcPQFpn ; player = https://www.youtube.com/s/player/f8071a08/player_ias.vflset/en_US/base.js\n"
     ]
    }
   ],
   "source": [
    "def fetch_video_ids_from_csv(csv_file_path):\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Extract video URLs from all columns\n",
    "        video_urls = []\n",
    "        seen_urls = set()  # To avoid duplicates\n",
    "        ydl_opts = {\n",
    "            'quiet': True,\n",
    "            'skip_download': True,\n",
    "        }\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            for column in df.columns:\n",
    "                cell_value = row[column]\n",
    "                if isinstance(cell_value, str):\n",
    "                    video_urls_in_cell = re.findall(\n",
    "                        r\"(https?://www\\.youtube\\.com/watch\\?v=[a-zA-Z0-9_-]{11})\", cell_value)\n",
    "                    for video_url in video_urls_in_cell:\n",
    "                        if video_url not in seen_urls:\n",
    "                            seen_urls.add(video_url)\n",
    "                            video_id_match = re.search(\n",
    "                                r\"v=([a-zA-Z0-9_-]{11})\", video_url)\n",
    "                            if video_id_match:\n",
    "                                video_id = video_id_match.group(1)\n",
    "\n",
    "                                # Fetch video title using yt-dlp\n",
    "                                with YoutubeDL(ydl_opts) as ydl:\n",
    "                                    info_dict = ydl.extract_info(\n",
    "                                        video_url, download=False)\n",
    "                                    video_title = info_dict.get('title', 'Unknown Title').replace(\n",
    "                                        '/', '_').replace('\\\\', '_')\n",
    "\n",
    "                                video_urls.append(\n",
    "                                    {\"ID\": video_id, \"URL\": video_url, \"Title\": video_title})\n",
    "\n",
    "        return video_urls\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV file: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def print_video_urls(video_urls):\n",
    "    df = pd.DataFrame(video_urls)\n",
    "    display(df)\n",
    "\n",
    "\n",
    "csv_file_path = \"/home/azureuser/cloudfiles/code/Users/Akilesh_Jayakumar/youtube-api/csv-files/New 10hrs From YT.csv\"\n",
    "video_urls = fetch_video_ids_from_csv(csv_file_path)\n",
    "print_video_urls(video_urls)\n",
    "print(\"Hello\")\n",
    "print(\"Hello\")\n",
    "print(\"Hello\")\n",
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Download and Convert Audio from YouTube Videos\n",
    "\n",
    "This function, `fetch_audio_for_all_videos`, downloads the audio from a list of YouTube video URLs and converts it to WAV format. The function performs the following steps:\n",
    "\n",
    "1. **Ensure Directory Exists**: Ensures that the specified result directory exists. If it doesn't, it creates the directory.\n",
    "2. **Iterate Through Video URLs**: Iterates through the provided list of video URLs.\n",
    "3. **Download and Convert Audio**: For each video URL, uses `yt-dlp` to download the audio in the best available format and converts it to WAV format.\n",
    "4. **Handle Errors**: Catches and prints any errors that occur during the download process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_audio_for_all_videos(video_urls, result_dir):\n",
    "    # Ensure the result directory exists\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    for video in video_urls:\n",
    "        video_url = video[\"URL\"]\n",
    "\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio/best',\n",
    "            'outtmpl': os.path.join(result_dir, '%(title)s.%(ext)s'),\n",
    "            'postprocessors': [{\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'wav',\n",
    "                'preferredquality': '192',\n",
    "            }],\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with YoutubeDL(ydl_opts) as ydl:\n",
    "                info_dict = ydl.extract_info(video_url, download=True)\n",
    "                title = info_dict.get('title', 'audio').replace(\n",
    "                    '/', '_').replace('\\\\', '_')\n",
    "                wav_path = os.path.join(result_dir, f\"{title}.wav\")\n",
    "                print(\n",
    "                    f\"Audio successfully downloaded and converted to WAV: {wav_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading audio for video {video_url}: {e}\")\n",
    "\n",
    "\n",
    "result_dir = \"/home/azureuser/cloudfiles/code/Users/Akilesh_Jayakumar/youtube-api/audio\"\n",
    "fetch_audio_for_all_videos(video_urls, result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Fetch and Save Transcripts\n",
    "\n",
    "This function fetches the transcript of a given video and saves it to a file. It supports multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def fetch_and_save_transcript(video_id, file_name, language):\n",
    "# #     def picker_trans(language):\n",
    "# #         if language == \"english\":\n",
    "# #             return \"en\"\n",
    "# #         elif language == \"chinese\":\n",
    "# #             return \"zh\"\n",
    "# #         elif language == \"malay\":\n",
    "# #             return \"ms\"\n",
    "# #     try:\n",
    "# #         transcript = YouTubeTranscriptApi.get_transcript(\n",
    "# #             video_id, languages=[picker_trans(language)])\n",
    "# #     except Exception as e:\n",
    "# #         print(f\"An error occurred: {e}\")\n",
    "# #         return False\n",
    "# #     with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "# #         for line in transcript:\n",
    "# #             file.write(f\"{line['text']}\\n\")\n",
    "# #     return True\n",
    "\n",
    "# # transscript = fetch_and_save_transcript(video_urls[1][\"ID\"], \"transcript.txt\", \"english\")\n",
    "# # print(transscript)\n",
    "\n",
    "# def fetch_and_save_transcripts(video_urls, output_dir, language):\n",
    "#     def pick_lang(language):\n",
    "#         if language == \"english\":\n",
    "#             return \"en\"\n",
    "#         elif language == \"chinese\":\n",
    "#             return \"zh\"\n",
    "#         elif language == \"malay\":\n",
    "#             return \"ms\"\n",
    "#         elif language == \"tamil\":\n",
    "#             return \"ta\"\n",
    "\n",
    "#     def format_timestamp(seconds):\n",
    "#         ms = int((seconds - int(seconds)) * 1000)\n",
    "#         hours, remainder = divmod(int(seconds), 3600)\n",
    "#         minutes, seconds = divmod(remainder, 60)\n",
    "#         return f\"{hours:02}:{minutes:02}:{seconds:02},{ms:03}\"\n",
    "\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     for video in video_urls:\n",
    "#         video_url = video[\"URL\"]\n",
    "\n",
    "#         # Extract video ID from URL\n",
    "#         video_id_match = re.search(r\"v=([a-zA-Z0-9_-]{11})\", video_url)\n",
    "#         if not video_id_match:\n",
    "#             print(f\"Invalid video URL: {video_url}\")\n",
    "#             continue\n",
    "\n",
    "#         video_id = video_id_match.group(1)\n",
    "\n",
    "#         try:\n",
    "#             # Extract video info using yt-dlp to get the title\n",
    "#             ydl_opts = {\n",
    "#                 'quiet': True,\n",
    "#                 'skip_download': True,\n",
    "#             }\n",
    "\n",
    "#             with YoutubeDL(ydl_opts) as ydl:\n",
    "#                 info_dict = ydl.extract_info(video_url, download=False)\n",
    "#                 title = info_dict.get('title', 'transcript').replace(\n",
    "#                     '/', '_').replace('\\\\', '_')\n",
    "\n",
    "#             transcript = YouTubeTranscriptApi.get_transcript(\n",
    "#                 video_id, languages=[pick_lang(language)])\n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred for {video_url}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         # Save the transcript in .srt format\n",
    "#         file_path = os.path.join(output_dir, f\"{title}.srt\")\n",
    "#         with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "#             for i, line in enumerate(transcript):\n",
    "#                 start = format_timestamp(line['start'])\n",
    "#                 duration = line['start'] + line['duration']\n",
    "#                 end = format_timestamp(duration)\n",
    "#                 text = line['text']\n",
    "#                 file.write(f\"{i + 1}\\n{start} --> {end}\\n{text}\\n\\n\")\n",
    "\n",
    "#         print(f\"Successfully saved transcript for {video_url}\")\n",
    "\n",
    "# output_dir = \"transcripts\"\n",
    "# fetch_and_save_transcripts(video_urls, output_dir, \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "\n",
    "# Path to the audio file to be transcribed\n",
    "audio = \"/home/azureuser/cloudfiles/code/Users/Akilesh_Jayakumar/youtube-api/audio/Asking Chennai Youngsters How Much They Earn   ｜ Street Interview ｜ Tamil ｜ Suman Mpm.wav\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize the transcription pipeline\n",
    "transcribe = pipeline(task=\"automatic-speech-recognition\", model=\"vasista22/whisper-tamil-medium\", chunk_length_s=30, device=device)\n",
    "transcribe.model.config.forced_decoder_ids = transcribe.tokenizer.get_decoder_prompt_ids(language=\"ta\", task=\"transcribe\")\n",
    "\n",
    "# Transcribe the audio file\n",
    "tamil_text = transcribe(audio)[\"text\"]\n",
    "print('Transcription: ', tamil_text)\n",
    "\n",
    "# Convert Tamil text to Romanized Tamil\n",
    "romanized_tamil = transliterate(tamil_text, sanscript.TAMIL, sanscript.ITRANS)\n",
    "print('Romanized Transcription: ', romanized_tamil)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
